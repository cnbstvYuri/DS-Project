"""
Pipeline de Treinamento e Serializa√ß√£o de Modelos.

Este script √© respons√°vel por todo o ciclo de vida do treinamento:
1. Carregamento e Sanitiza√ß√£o de Dados (Limpeza).
2. Corre√ß√£o de Labels (Target Inversion).
3. Engenharia de Atributos (via src.utils).
4. Defini√ß√£o de Pipelines de Pr√©-processamento (Imputa√ß√£o + Scaling).
5. Treinamento de Modelos (Random Forest e Logistic Regression).
6. Serializa√ß√£o (.joblib) para uso em produ√ß√£o.

Uso:
    python src/train_and_save.py --data data/heart.csv --target target
"""

import argparse
import os
import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# Importa a l√≥gica centralizada de engenharia de features para garantir consist√™ncia
# com o ambiente de produ√ß√£o (app.py)
from utils import feature_engineering

# Dicion√°rio de mapeamento para padronizar nomes de colunas
COL_MAP = {
    'age':'age','sex':'sex','cp':'cp','trestbps':'resting_bp','chol':'chol','fbs':'fbs','restecg':'restecg',
    'thalach':'thalach','exang':'exang','oldpeak':'oldpeak','slope':'slope','ca':'ca','thal':'thal','target':'target'
}

def load_and_sanitize_data(path):
    """
    Carrega o dataset e aplica regras de qualidade de dados (Data Quality).
    Remove duplicatas e filtra valores fisiologicamente imposs√≠veis (Outliers de erro).
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset n√£o encontrado em: {path}")
        
    print(f"üìÇ Carregando dataset: {path}")
    df = pd.read_csv(path)
    
    # 1. Padroniza√ß√£o de Schema
    df = df.rename(columns={k:v for k,v in COL_MAP.items() if k in df.columns})
    
    # 2. Remo√ß√£o de Duplicatas
    # Dados duplicados causam vazamento de dados (data leakage) entre treino e teste.
    n_total = len(df)
    df = df.drop_duplicates()
    n_removidos = n_total - len(df)
    if n_removidos > 0:
        print(f"üßπ Data Cleaning: {n_removidos} linhas duplicadas removidas.")
    
    # 3. Sanity Check (Remo√ß√£o de Erros de Digita√ß√£o)
    # Ex: 'oldpeak' > 20 √© imposs√≠vel clinicamente.
    if 'oldpeak' in df.columns:
        mask_outlier = df['oldpeak'] > 20 
        if mask_outlier.sum() > 0:
            print(f"‚ö†Ô∏è Outliers: Removendo {mask_outlier.sum()} registros com erro em 'oldpeak'.")
            df = df[~mask_outlier]

    return df

def get_feature_lists(X: pd.DataFrame):
    """
    Separa automaticamente colunas num√©ricas e categ√≥ricas baseada em heur√≠sticas.
    Colunas num√©ricas com baixa cardinalidade (<10 valores √∫nicos) s√£o tratadas como categ√≥ricas.
    """
    # Heur√≠stica: Num√©ricos com poucos valores √∫nicos (ex: slope 0,1,2) viram categ√≥ricos
    potential_cat = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c]) and X[c].nunique() < 10]
    
    # Num√©ricos puros
    num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()
    
    # Remove os "falsos num√©ricos" da lista
    for c in potential_cat:
        if c in num_cols:
            num_cols.remove(c)
            
    # Categ√≥ricos finais
    cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist() + potential_cat
    cat_cols = list(dict.fromkeys(cat_cols)) # Deduplica√ß√£o
    
    # Garante integridade
    num_cols = [c for c in num_cols if c not in cat_cols]
    
    return num_cols, cat_cols

def main(args):
    # 1. ETL Inicial
    df = load_and_sanitize_data(args.data)
    
    # -----------------------------------------------------------
    # PADRONIZA√á√ÉO DO TARGET (Crucial para Interpretabilidade)
    # An√°lise explorat√≥ria indicou invers√£o no dataset original (0=Doen√ßa).
    # Invertemos aqui para garantir que 1=Doen√ßa (Padr√£o Positivo).
    # -----------------------------------------------------------
    if 'target' in df.columns:
        print("üîÑ Normaliza√ß√£o: Ajustando Target para padr√£o (1 = Doen√ßa Detectada)...")
        df['target'] = df['target'].apply(lambda x: 1 if x == 0 else 0)
        print(f"   Distribui√ß√£o de Classes: {df['target'].value_counts().to_dict()}")
    
    # 2. Feature Engineering
    # Utiliza a fun√ß√£o centralizada do utils para manter paridade com o Dashboard
    df = feature_engineering(df)
    
    # Valida√ß√£o de Colunas
    TARGET = args.target
    if TARGET not in df.columns:
        raise ValueError(f"Coluna alvo '{TARGET}' n√£o encontrada no dataset.")
    
    # Sele√ß√£o de Features (White-list)
    # Garante que apenas colunas conhecidas entrem no modelo
    features_list = [
        'age','sex','cp','resting_bp','chol','fbs','restecg','thalach','exang',
        'oldpeak','slope','ca','thal','AgeGroup','CholCategory','AgeOver50','CSI','RiskFactorsCount'
    ]
    features_final = [f for f in features_list if f in df.columns]
    
    X = df[features_final].copy()
    y = df[TARGET].copy()
    
    num_cols, cat_cols = get_feature_lists(X)
    print(f'‚öôÔ∏è Setup: {len(num_cols)} features num√©ricas | {len(cat_cols)} features categ√≥ricas')
    
    # 3. Defini√ß√£o do Pr√©-processamento (Pipeline Robusto)
    
    # Pipeline Num√©rico: Imputa√ß√£o pela Mediana (para robustez a nulos) + Normaliza√ß√£o Z-Score
    num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Pipeline Categ√≥rico: Imputa√ß√£o de Constante + OneHotEncoding
    cat_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    preprocessor = ColumnTransformer([
        ('num', num_pipeline, num_cols),
        ('cat', cat_pipeline, cat_cols)
    ])
    
    # 4. Defini√ß√£o dos Modelos
    
    # Random Forest: Robusto para n√£o-linearidades e intera√ß√µes
    rf = Pipeline([
        ('preproc', preprocessor), 
        ('model', RandomForestClassifier(
            n_estimators=args.n_estimators, 
            random_state=42, 
            max_depth=args.rf_max_depth, 
            min_samples_split=args.rf_min_samples_split, 
            min_samples_leaf=args.rf_min_samples_leaf
        ))
    ])
    
    # Regress√£o Log√≠stica: Baseline interpret√°vel e probabil√≠stica
    lr = Pipeline([
        ('preproc', preprocessor), 
        ('model', LogisticRegression(max_iter=1000, solver='lbfgs'))
    ])
    
    # 5. Split de Dados
    # Stratify=y garante que a propor√ß√£o de doentes seja igual no treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=42, stratify=y)
    
    # 6. Treinamento
    print('üöÄ Iniciando treinamento dos modelos...')
    rf.fit(X_train, y_train)
    lr.fit(X_train, y_train)
    
    # 7. Serializa√ß√£o (Salvamento)
    os.makedirs('models', exist_ok=True)
    
    # Salvamos os modelos treinados
    joblib.dump(rf, 'models/rf_model.joblib')
    joblib.dump(lr, 'models/lr_model.joblib')
    # Salvamos o preprocessor separadamente (√∫til para SHAP/Explainer)
    joblib.dump(preprocessor, 'models/preprocessor.joblib')
    
    # Salvamos o conjunto de teste para avalia√ß√£o honesta no Dashboard
    X_test.to_csv('models/X_test.csv', index=False)
    pd.Series(y_test).to_csv('models/y_test.csv', index=False, header=['target'])
    
    print('‚úÖ Pipeline conclu√≠do. Artefatos salvos no diret√≥rio models/.')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Pipeline de Treinamento Heart Disease Prediction")
    parser.add_argument('--data', type=str, required=True, help='Caminho para o CSV do dataset')
    parser.add_argument('--target', type=str, default='target', help='Nome da coluna alvo')
    parser.add_argument('--test_size', type=float, default=0.3, help='Propor√ß√£o do conjunto de teste')
    
    # Hiperpar√¢metros RF
    parser.add_argument('--n_estimators', type=int, default=300)
    parser.add_argument('--rf_max_depth', type=int, default=7)
    parser.add_argument('--rf_min_samples_split', type=int, default=20)
    parser.add_argument('--rf_min_samples_leaf', type=int, default=10)
    
    args = parser.parse_args()
    main(args)